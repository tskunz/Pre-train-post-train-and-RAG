## Final Exam Guidelines

Pre-train Embedding Models, Post-train Language Model, and Build a RAG System

### Part 1: Pre-training Embedding Models

Students will pre-train **TWO embedding models from scratch** (100m-200m parameters each):

1. **General-purpose embedding model**: Trained on a general corpus (e.g., Wikipedia, BookCorpus, C4, or similar)
2. **Paul Graham-specific embedding model**: Trained specifically on Paul Graham essays to create domain-specialized embeddings

#### Requirements:

- **Model Size**: 100m-200m parameters for each model

- **Loss Plots**: Provide train and validation loss plots for both models

- **Training Configuration Documentation**: Explain your training configs including:
  - Learning Rate
  - Optimizer (e.g., AdamW, Adam)
  - LR Schedule (e.g., linear warmup + cosine decay)
  - Batch size
  - Training steps/epochs
  - Contrastive learning parameters (temperature, negative samples)

- **Architecture Comparison**: Compare 2 different embedding model architectures/approaches:
  - Options include: BERT-style encoders, Sentence Transformers, bi-encoders, cross-encoders, contrastive learning methods (SimCSE, InfoNCE), etc.
  - **Justify your architecture choice** (2 paragraphs): Explain the design decisions, trade-offs between architectures, and why you selected each approach
  
- **Embedding-Specific Metrics**: Report contrastive loss, and if possible, retrieval metrics (MRR, nDCG) on a validation set

- **Comparison Analysis**: Compare the performance of your general vs. Paul Graham-specific embedding model on a held-out retrieval task

---

### Part 2: Post-Training Language Model

Use **Gemma-3-1b-it** (a pre-trained instruction-tuned language model) and post-train it on Paul Graham essay data for question-answering tasks.

#### Requirements:

- **Base Model**: Start with Gemma-3-1b-it (available on HuggingFace)

- **Dataset**: Generate synthetic QnA pairs using the Paul Graham corpus 
  
- **Gen AI Strategy**: Explain your post-training strategy (2 paragraphs):
  - Why you chose your specific fine-tuning approach (LoRA, full fine-tuning, prompt tuning, etc.)
  - How it works technically and why it's appropriate for this task
  
- **Training Comparison**: Compare three training approaches:
  1. **Synthetic data only**: QnA pairs generated by LLMs
  2. **Base QnA only**: Manually created or extracted QnA pairs
  3. **Both**: Combined synthetic + base QnA data
  
- **Evaluation Metrics**: Report the following on validation set (20% of the data):
  - Loss
  - BERT-F1 (semantic similarity)
  - ROUGE-L (lexical overlap)
  - F1 Score (token-level)

- **Best Model Selection**: Identify which training approach works best and explain why

---

### Part 3: RAG System Benchmark

Build and evaluate a complete Retrieval-Augmented Generation (RAG) system using the Paul Graham essay corpus.

#### System Components:

- **Retrieval Component**: Use your **pre-trained embedding models** (from Part 1)
  - Compare both your general-purpose and Paul Graham-specific embedding models
  - Do NOT use external embedding models (e.g., Gemma3-300m, OpenAI embeddings)
  
- **Generation Component**: Use your **post-trained Gemma-3-1b-it** model (from Part 2)

- **Knowledge Base**: Paul Graham essays corpus (see Dataset section below)

#### Requirements:

- **RAG Pipeline Implementation**:
  - Chunk Paul Graham essays appropriately for retrieval
  - Implement semantic search using your embedding models
  - Retrieve top-k relevant passages for each query
  - Generate answers using retrieved context + Gemma-3-1b-it

- **Embedding Model Comparison**:
  - Compare retrieval performance between:
    - Your general-purpose embedding model
    - Your Paul Graham-specific embedding model
  - Report retrieval metrics: Precision@k, Recall@k, MRR

- **Generation Quality Evaluation**:
  - Answer benchmark questions about Paul Graham essays (provided by instructors)
  - Compare your system's outputs with OpenAI GPT-4.1-mini.
  
- **Benchmark Evaluation**: Your responses will be evaluated on a holdout test set using:
  - Answer accuracy
  - Relevance scores
  - Semantic similarity to ground truth
  - Factual consistency with source material

- **Analysis**: Provide analysis (2-3 paragraphs) explaining:
  - Which embedding model performed better for retrieval and why
  - How your system compares to frontier models
  - Limitations and potential improvements

---

## Paul Graham Dataset Information

### Dataset Overview

Paul Graham is a renowned programmer, writer, and co-founder of Y Combinator. His essays cover topics including startups, technology, programming, venture capital, philosophy, and education. The corpus consists of over 200 essays published from 2001 to present, making it an excellent domain-specific dataset for training and evaluation.

### Dataset Sources

**Official Source:**
- Paul Graham's official website: http://www.paulgraham.com/articles.html
- Direct access to all essays in HTML format

**Datasets and Codes:**
- `pcuenq/paul-graham-essays`: Pre-processed collection of essays
  - Link: https://github.com/TonicAI/tonic_validate/tree/main

## Submission Requirements

- Submit all code (training scripts, evaluation scripts, RAG pipeline)
- Include comprehensive documentation explaining your approach
- Provide all required plots, metrics, and comparisons
- Write clear analysis sections for each part
- Include a README with setup and running instructions
- Ensure reproducibility with requirements.txt or environment.yml
